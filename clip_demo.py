import time
import cv2
import torch
import tkinter as tk
from PIL import Image, ImageTk
from threading import Thread, Lock, Event
from queue import Queue, LifoQueue
from ultralytics import YOLO
from transformers import CLIPProcessor, CLIPModel
import numpy as np

from utils import PlaceholderEntry, Translator, cv2_puttext_chinese, letterbox
from negative_text_gen import NegativeTextGenerator

device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")

# å…¨å±€é…ç½®
MAX_FPS = 25       # é™åˆ¶æœ€å¤§å¸§ç‡
FRAME_SCALE = 0.5  # ç”»é¢ç¼©æ”¾æ¯”ä¾‹
DETECT_INTERVAL = 8  # æ¯3å¸§è¿›è¡Œä¸€æ¬¡æ£€æµ‹
PROBABILITY = 0.7
MAX_OBJECTS_TO_PROCESS = 40  # é™åˆ¶æ¯å¸§å¤„ç†çš„æœ€å¤§ç›®æ ‡æ•°
BOX_DISPLAY_DURATION = 1.0  # æ–°å¢ï¼šæ£€æµ‹æ¡†æ˜¾ç¤ºçš„æœ€å¤§æ—¶é—´ï¼ˆç§’ï¼‰
VIDEO_PATH = 0
# VIDEO_PATH = "/Users/Larry/Downloads/47516-451623701_medium.mp4"
# VIDEO_PATH = "/Users/Larry/Desktop/ç›®æ ‡æ£€æµ‹æ•ˆæœå±•ç¤º/è§†é¢‘æ¼”ç¤º/ç¾å›½ç‹©çŒç›¸æœº/source/test_video/Vehicle_01.mp4"
# VIDEO_PATH = "/Users/Larry/Downloads/2053100-hd_1920_1080_30fps.mp4"

# å…±äº«èµ„æº
input_queue = Queue()
frame_queue = Queue(maxsize=3)    # åŸå§‹å¸§é˜Ÿåˆ—
yolo_queue = Queue(maxsize=1)     # YOLOå¤„ç†é˜Ÿåˆ—
clip_queue = Queue(maxsize=40)    # CLIPå¤„ç†é˜Ÿåˆ—
clip_tasks_queue = LifoQueue(maxsize=1)  # æ–°å¢ï¼šCLIPä»»åŠ¡é˜Ÿåˆ—ï¼Œåªä¿å­˜æœ€æ–°ä¸€å¸§çš„ä»»åŠ¡
result_queue = Queue(maxsize=1000)   # ç»“æœé˜Ÿåˆ—
display_queue = Queue(maxsize=2)  # ç”¨äºæ˜¾ç¤ºçš„å¸§é˜Ÿåˆ—
exit_event = Event()
new_yolo_result = Event()  # æ–°å¢ï¼šæ ‡è®°æœ‰æ–°çš„YOLOç»“æœ

# æ·»åŠ é”æ¥ä¿æŠ¤ç»“æœé˜Ÿåˆ—å’Œæ£€æµ‹å¯¹è±¡
result_lock = Lock()
objects_lock = Lock()
clip_queue_lock = Lock()  # æ–°å¢ï¼šä¿æŠ¤CLIPé˜Ÿåˆ—çš„é”

# æ¨¡å‹åŠ è½½
yolo = YOLO("yolov8n.pt").to(device)
clip_model = CLIPModel.from_pretrained("/Users/Larry/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268").to(device)
clip_processor = CLIPProcessor.from_pretrained("/Users/Larry/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268")
translate_model = Translator()
generator = NegativeTextGenerator()

# æ€§èƒ½ç»Ÿè®¡
stats_lock = Lock()
perf_stats = {
    "yolo_fps": "N/A",
    "clip_fps": "N/A",
    "total_fps": "N/A",
    "objects_processed": 0,
    "skipped_frames": 0,  # æ–°å¢ï¼šè®°å½•è·³è¿‡çš„å¸§æ•°
    "frame_process_time": "N/A",  # æ–°å¢ï¼šæ¯å¸§å¤„ç†æ—¶é—´
    "last_frame_objects": 0       # æ–°å¢ï¼šä¸Šä¸€å¸§å¤„ç†çš„å¯¹è±¡æ•°é‡
}

# æ·»åŠ å¸§ä»»åŠ¡åè°ƒç»“æ„
frame_tasks = {}
frame_tasks_lock = Lock()


class MainApplication:
    def __init__(self):
        # ä¸»çª—å£
        self.root = tk.Tk()
        self.root.title("æ™ºèƒ½ç›‘æ§ç³»ç»Ÿ")
        self.root.protocol("WM_DELETE_WINDOW", self.on_close)
        self.root.resizable(False, False)

        self.entry_target_text = '' # è¾“å…¥æ¡†æ–‡æœ¬
        self.current_target_text = '' # å½“å‰æœç´¢ç›®æ ‡æ–‡æœ¬
        self.negative_texts = [] # è´Ÿæ–‡æœ¬åˆ—è¡¨

        self.entry_var = tk.StringVar() # è¾“å…¥æ¡†æ–‡æœ¬å˜é‡
        self.target_var = tk.StringVar(value=self.current_target_text) # å½“å‰æœç´¢ç›®æ ‡æ–‡æœ¬å˜é‡
        self.negative_var = tk.StringVar(value=self.negative_texts) # è´Ÿæ ·æœ¬æ–‡æœ¬å˜é‡
        
        # è§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
        self.set_video_frame()
        
        # æ§åˆ¶é¢æ¿
        self.set_control_frame()
        
        # å¯åŠ¨å¼‚æ­¥å¤„ç†çº¿ç¨‹
        self.start_processing_threads()
        self.root.after(40, self.update_video)  # 25fpsæ›´æ–°

        # æ·»åŠ æ£€æµ‹ç»“æœç¼“å­˜ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š((x1, y1, x2, y2), label, timestamp)
        self.detected_objects = []
        self.last_detection_time = time.time()


    def set_video_frame(self):
        self.video_frame = tk.Label(self.root)
        self.video_frame.pack(padx=10, pady=10, side=tk.BOTTOM)


    def set_control_frame(self):
        control_frame = tk.Frame(self.root, height=100)
        control_frame.pack(padx=10, pady=10, side=tk.BOTTOM, fill=tk.BOTH, expand=True)
        
        # è¾“å…¥ç»„ä»¶
        entry = PlaceholderEntry(control_frame, textvariable=self.entry_var, width=20, placeholder="è¯·è¾“å…¥æœç´¢çš„ç›®æ ‡ ğŸ”", font=("Arial", 20))
        entry.place(x=10, y=10)

        # æŒ‰é’®
        button = tk.Button(control_frame, text="æœç´¢", command=self.update_target, font=("Arial", 25))
        button.place(x=100, y=60)
        
        # æ€§èƒ½ç›‘æ§
        self.stats_var = tk.StringVar()
        stats_label = tk.Label(control_frame, textvariable=self.stats_var, justify=tk.RIGHT, font=("Arial", 12))
        stats_label.place(relx=1.0, rely=0.0, anchor=tk.NE)

        # å½“å‰æœç´¢ç›®æ ‡çš„æ ‡ç­¾
        current_target_label = tk.Label(control_frame, text="å½“å‰æœç´¢ç›®æ ‡:", font=("Arial", 20))
        current_target_label.place(relx=0.3, rely=0.1, anchor=tk.NW)

        # æ˜¾ç¤ºå½“å‰æœç´¢ç›®æ ‡çš„å€¼
        current_target_display = tk.Label(control_frame, textvariable=self.target_var, font=("Arial", 20))
        current_target_display.place(relx=0.3, rely=0.4, anchor=tk.NW)

        # æ˜¾ç¤ºè´Ÿæ ·æœ¬æ ‡é¢˜
        negative_samples_label = tk.Label(control_frame, text="è´Ÿæ ·æœ¬:", font=("Arial", 20))
        negative_samples_label.place(relx=0.5, rely=0.1, anchor=tk.NW)

        # æ˜¾ç¤ºè´Ÿæ ·æœ¬å†…å®¹
        negative_samples_display = tk.Label(control_frame, textvariable=self.negative_var, 
                                           font=("Arial", 14), justify=tk.LEFT, wraplength=300)
        negative_samples_display.place(relx=0.6, rely=0.15, anchor=tk.NW)

        # æ›´æ–°è´Ÿæ ·æœ¬æ–¹æ³•
        def update_negative_samples():
            if self.negative_texts:
                # åªå–å‰3ä¸ªè´Ÿæ ·æœ¬æ˜¾ç¤º
                neg_text = '\n'.join(self.negative_texts[:3])
                self.negative_var.set(neg_text)
            else:
                self.negative_var.set("æ— è´Ÿæ ·æœ¬")
                
        update_negative_samples()  # åˆå§‹åŒ–æ˜¾ç¤º

    def start_processing_threads(self):
        """å¯åŠ¨æ‰€æœ‰å¤„ç†çº¿ç¨‹"""
        # 1. è§†é¢‘é‡‡é›†çº¿ç¨‹
        Thread(target=self.frame_capture_worker, daemon=True).start()
        
        # 2. YOLOæ£€æµ‹çº¿ç¨‹
        Thread(target=self.yolo_worker, daemon=True).start()
        
        # 3. CLIPè°ƒåº¦å™¨çº¿ç¨‹ - æ–°å¢
        Thread(target=self.clip_scheduler, daemon=True).start()
        
        # 4. CLIPå¤„ç†çº¿ç¨‹ - åˆ›å»ºå¤šä¸ªçº¿ç¨‹ä»¥æé«˜å¹¶è¡Œåº¦
        for _ in range(4):  # å¯æ ¹æ®ç¡¬ä»¶è°ƒæ•´çº¿ç¨‹æ•°
            Thread(target=self.clip_worker, daemon=True).start()
            
        # 5. ç»“æœå¤„ç†ä¸æ¸²æŸ“çº¿ç¨‹
        Thread(target=self.render_worker, daemon=True).start()


    def update_target(self):
        if new_text := self.entry_var.get().strip():
            new_text_with_prompt = f"ä¸€å¼ {new_text}çš„ç…§ç‰‡"
            en_text = translate_model.translate(new_text_with_prompt, 'zh2en')[0]
            self.negative_texts = generator.generate(en_text, top_n=5)
            print(f"è‹±æ–‡æ–‡æœ¬ï¼š{en_text}")
            print(f"è´Ÿæ–‡æœ¬ï¼š{self.negative_texts}")
            input_queue.put(en_text)
            self.negative_var.set("\n".join(self.negative_texts))
            self.target_var.set(self.entry_var.get())

    def frame_capture_worker(self):
        """çº¿ç¨‹1: è´Ÿè´£è§†é¢‘å¸§é‡‡é›†"""
        cap = cv2.VideoCapture(VIDEO_PATH)
        last_time = time.time()
        frame_count = 0

        try:
            while not exit_event.is_set():
                # å¸§ç‡æ§åˆ¶
                elapsed = time.time() - last_time
                if elapsed < 1 / MAX_FPS: 
                    time.sleep(0.001)
                    continue
                
                # è®¡ç®—æ€»ä½“FPS
                with stats_lock:
                    perf_stats["total_fps"] = f"{1 / elapsed:.1f}"
                
                last_time = time.time()
                
                # è¯»å–å¸§
                ret, original_frame = cap.read()
                if not ret:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    continue

                # å¤„ç†è¾“å…¥æ›´æ–°
                if not input_queue.empty():
                    self.current_target_text = input_queue.get()
                
                # ç¼©æ”¾å¸§ï¼ˆå…ˆç¼©æ”¾ç”¨äºæ˜¾ç¤ºï¼‰
                scaled_frame = cv2.resize(original_frame, (0,0), fx=FRAME_SCALE, fy=FRAME_SCALE)
                
                # å°†åŸå§‹å¸§é€å…¥YOLOå¤„ç†é˜Ÿåˆ—ï¼ˆæ¯éš”å‡ å¸§ï¼‰
                frame_count += 1
                if frame_count % DETECT_INTERVAL == 0:
                    if not yolo_queue.full():  # é˜²æ­¢å¤„ç†è¿‡æ…¢å¯¼è‡´ç§¯å‹
                        yolo_queue.put((original_frame.copy(), scaled_frame.copy()))

                # æ”¾å…¥æ¸²æŸ“é˜Ÿåˆ—
                if not frame_queue.full():
                    frame_queue.put(scaled_frame)
        finally:
            cap.release()
    
    def yolo_worker(self):
        """çº¿ç¨‹2: è´Ÿè´£YOLOç›®æ ‡æ£€æµ‹"""
        while not exit_event.is_set():
            try:
                if yolo_queue.empty():
                    time.sleep(0.01)
                    continue
                    
                original_frame, scaled_frame = yolo_queue.get()
                
                # YOLOæ£€æµ‹
                yolo_start = time.time()
                results = yolo(original_frame, verbose=False, device=device)
                yolo_time = time.time() - yolo_start
                
                # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
                with stats_lock:
                    perf_stats["yolo_fps"] = f"{1 / yolo_time:.1f}"
                
                # æŒ‰ç½®ä¿¡åº¦æ’åºå¹¶é™åˆ¶å¤„ç†ç›®æ ‡æ•°é‡
                boxes = []
                for box in results[0].boxes:
                    conf = float(box.conf[0])
                    boxes.append((box.xyxy[0], conf))
                
                # æŒ‰ç½®ä¿¡åº¦é™åºæ’åº
                boxes.sort(key=lambda x: x[1], reverse=True)
                
                # åˆ›å»ºä¸€ä¸ªå¸§IDæ¥å…³è”åŒä¸€å¸§çš„æ‰€æœ‰ç›®æ ‡
                frame_id = int(time.time() * 1000)  # æ¯«ç§’çº§æ—¶é—´æˆ³ä½œä¸ºå¸§ID
                
                # è·å–å½“å‰å¸§è¦å¤„ç†çš„ç›®æ ‡æ•°é‡
                targets_count = min(len(boxes), MAX_OBJECTS_TO_PROCESS)
                
                if targets_count > 0:
                    # åˆå§‹åŒ–å¸§ä»»åŠ¡è®¡æ•°å™¨å’Œç»“æœåˆ—è¡¨
                    with frame_tasks_lock:
                        frame_tasks[frame_id] = {
                            'total': targets_count,  # æ€»ä»»åŠ¡æ•°
                            'completed': 0,          # å·²å®Œæˆä»»åŠ¡æ•°
                            'results': [],           # ç»“æœé›†åˆ
                            'scaled_frame': scaled_frame.copy(),  # ä¿å­˜å½“å‰å¸§ç”¨äºåç»­å¤„ç†
                            'start_time': None       # æ–°å¢ï¼šè®°å½•å¼€å§‹å¤„ç†æ—¶é—´
                        }
                    
                    # å‡†å¤‡å½“å‰å¸§çš„æ‰€æœ‰ç›®æ ‡å¤„ç†ä»»åŠ¡
                    frame_tasks_list = []
                    for i, (box_tensor, _) in enumerate(boxes[:MAX_OBJECTS_TO_PROCESS]):
                        if i >= targets_count:
                            break
                        
                        x1, y1, x2, y2 = map(int, box_tensor)
                        
                        # æå–ROIå¹¶è¿›è¡Œé¢„å¤„ç†
                        roi = original_frame[y1:y2, x1:x2]
                        if roi.size < 100:
                            # è·³è¿‡è¿‡å°çš„ç›®æ ‡
                            continue
                        
                        # ä¿å­˜è¯¥ç›®æ ‡çš„å¤„ç†ä»»åŠ¡
                        frame_tasks_list.append((roi.copy(), (x1, y1, x2, y2), frame_id, i))
                    
                    # æŠŠæ‰€æœ‰ä»»åŠ¡ä¸€æ¬¡æ€§æäº¤åˆ°è°ƒåº¦é˜Ÿåˆ—
                    if frame_tasks_list:
                        # æ¸…ç©ºæ—§çš„CLIPä»»åŠ¡é˜Ÿåˆ—
                        with clip_queue_lock:
                            while not clip_tasks_queue.empty():
                                try:
                                    clip_tasks_queue.get_nowait()
                                    with stats_lock:
                                        perf_stats["skipped_frames"] += 1
                                except:
                                    break
                            
                            # æäº¤æ–°çš„ä»»åŠ¡åˆ—è¡¨
                            clip_tasks_queue.put((frame_id, frame_tasks_list))
                        
                        # æ ‡è®°æœ‰æ–°çš„YOLOç»“æœç­‰å¾…å¤„ç†
                        new_yolo_result.set()
                
            except Exception as e:
                print(f"YOLOå¤„ç†é”™è¯¯: {e}")
                time.sleep(0.1)
    
    def clip_scheduler(self):
        """çº¿ç¨‹3: è´Ÿè´£è°ƒåº¦CLIPä»»åŠ¡ï¼Œç¡®ä¿åªå¤„ç†æœ€æ–°çš„å¸§"""
        while not exit_event.is_set():
            try:
                # ç­‰å¾…æ–°çš„YOLOç»“æœ
                new_yolo_result.wait(timeout=0.5)
                if exit_event.is_set():
                    break
                
                # æ¸…é™¤æ ‡å¿—
                new_yolo_result.clear()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ä»»åŠ¡
                if clip_tasks_queue.empty():
                    continue
                
                # è·å–æœ€æ–°çš„ä¸€å¸§ä»»åŠ¡
                frame_id, tasks_list = clip_tasks_queue.get()
                
                # è®°å½•å¼€å§‹å¤„ç†è¿™ä¸€å¸§çš„æ—¶é—´
                with frame_tasks_lock:
                    if frame_id in frame_tasks:
                        frame_tasks[frame_id]['start_time'] = time.time()
                        # è®°å½•è¿™ä¸€å¸§çš„ç›®æ ‡æ€»æ•°
                        with stats_lock:
                            perf_stats["last_frame_objects"] = len(tasks_list)
                
                # æ¸…ç©ºç°æœ‰çš„CLIPé˜Ÿåˆ—
                with clip_queue_lock:
                    if not clip_queue.empty():
                        continue
                    
                    # å°†æ–°ä»»åŠ¡æ·»åŠ åˆ°CLIPå¤„ç†é˜Ÿåˆ—
                    for task in tasks_list:
                        clip_queue.put(task)
                
            except Exception as e:
                print(f"CLIPè°ƒåº¦å™¨é”™è¯¯: {e}")
                time.sleep(0.1)
    
    def clip_worker(self):
        """çº¿ç¨‹4: è´Ÿè´£CLIPæ–‡æœ¬-å›¾åƒåŒ¹é…"""
        clip_times = []
        max_times = 10  # ç”¨äºè®¡ç®—å¹³å‡CLIPå¤„ç†æ—¶é—´
        
        while not exit_event.is_set():
            try:
                if clip_queue.empty():
                    time.sleep(0.01)
                    continue
                
                with clip_queue_lock:
                    if clip_queue.empty():
                        continue
                    roi, box_coords, frame_id, target_idx = clip_queue.get()
                
                # æ£€æŸ¥è¯¥å¸§æ˜¯å¦è¿˜åœ¨è·Ÿè¸ªä¸­
                with frame_tasks_lock:
                    if frame_id not in frame_tasks:
                        continue  # å¸§å·²ç»è¢«å¤„ç†æˆ–æ¸…ç†
                
                x1, y1, x2, y2 = box_coords
                
                # å°†åæ ‡è½¬æ¢ä¸ºç¼©æ”¾åçš„å°ºå¯¸
                x1_scaled = int(x1 * FRAME_SCALE)
                y1_scaled = int(y1 * FRAME_SCALE)
                x2_scaled = int(x2 * FRAME_SCALE)
                y2_scaled = int(y2 * FRAME_SCALE)
                
                # CLIPå¤„ç†
                clip_start = time.time()
                image = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))
                input_texts = [self.current_target_text] + self.negative_texts
                if len(input_texts) <= 1 and input_texts[0] == "":
                    # å¦‚æœæ²¡æœ‰è¾“å…¥æ–‡æœ¬ï¼Œåˆ™è·³è¿‡å¤„ç†
                    continue
                inputs = clip_processor(
                    text=input_texts,
                    images=image,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).to(device)
                
                with torch.no_grad():
                    outputs = clip_model(**inputs)
                
                clip_time = time.time() - clip_start
                clip_times.append(clip_time)
                if len(clip_times) > max_times:
                    clip_times.pop(0)
                
                # æ›´æ–°CLIPå¤„ç†æ€§èƒ½ç»Ÿè®¡
                with stats_lock:
                    perf_stats["clip_fps"] = f"{1 / (sum(clip_times) / len(clip_times)):.1f}"
                    perf_stats["objects_processed"] += 1
                
                probs = outputs.logits_per_image.softmax(dim=1).squeeze()
                
                # æ›´æ–°å¸§ä»»åŠ¡çŠ¶æ€å¹¶æ£€æŸ¥æ˜¯å¦æ˜¯è¯¥å¸§çš„æœ€åä¸€ä¸ªä»»åŠ¡
                with frame_tasks_lock:
                    if frame_id in frame_tasks:
                        # å¦‚æœç›®æ ‡æ¦‚ç‡è¶…è¿‡é˜ˆå€¼ï¼Œå°†ç»“æœæ·»åŠ åˆ°è¯¥å¸§çš„ç»“æœåˆ—è¡¨ä¸­
                        if probs[0].item() > PROBABILITY:
                            # æ·»åŠ æ—¶é—´æˆ³åˆ°ç»“æœä¸­
                            current_time = time.time()
                            result_item = ((x1_scaled, y1_scaled, x2_scaled, y2_scaled), # ç›®æ ‡æ¡†åæ ‡
                                          self.target_var.get().strip(), # å½“å‰æœç´¢ç›®æ ‡æ–‡æœ¬
                                          probs[0].item(), #  # ç½®ä¿¡åº¦
                                          current_time)  # æ·»åŠ æ—¶é—´æˆ³
                            frame_tasks[frame_id]['results'].append(result_item)
                        
                        # æ›´æ–°å®Œæˆä»»åŠ¡è®¡æ•°
                        frame_tasks[frame_id]['completed'] += 1
                        
                        # æ£€æŸ¥è¯¥å¸§çš„æ‰€æœ‰ç›®æ ‡æ˜¯å¦éƒ½å·²å¤„ç†å®Œæ¯•
                        if frame_tasks[frame_id]['completed'] >= frame_tasks[frame_id]['total']:
                            # è®¡ç®—è¯¥å¸§å¤„ç†çš„æ€»æ—¶é—´
                            if 'start_time' in frame_tasks[frame_id]:
                                frame_process_time = time.time() - frame_tasks[frame_id]['start_time']
                                with stats_lock:
                                    perf_stats["frame_process_time"] = f"{frame_process_time:.3f}"
                                print(f"å¸§ {frame_id} å¤„ç†å®Œæˆï¼Œå…± {frame_tasks[frame_id]['total']} ä¸ªç›®æ ‡ï¼Œè€—æ—¶: {frame_process_time:.3f}ç§’")
                            
                            # æ‰€æœ‰ç›®æ ‡éƒ½å·²å¤„ç†å®Œï¼Œå¯ä»¥æäº¤ç»“æœ
                            with result_lock:
                                result_queue.put(frame_tasks[frame_id]['results'])
                            # æ¸…ç†è¯¥å¸§çš„ä»»åŠ¡çŠ¶æ€
                            del frame_tasks[frame_id]
                
            except Exception as e:
                print(f"CLIPå¤„ç†é”™è¯¯: {e}")
                # å¦‚æœå‡ºé”™ï¼Œä¹Ÿè¦æ›´æ–°è®¡æ•°ï¼Œé¿å…å¡ä½
                try:
                    with frame_tasks_lock:
                        if frame_id in frame_tasks:
                            frame_tasks[frame_id]['completed'] += 1
                except:
                    pass
                time.sleep(0.1)
    
    def render_worker(self):
        """çº¿ç¨‹5: è´Ÿè´£æ¸²æŸ“å’Œç»“æœåˆæˆ"""
        while not exit_event.is_set():
            try:
                # è·å–æœ€æ–°çš„æ¸²æŸ“å¸§
                if frame_queue.empty():
                    time.sleep(0.01)
                    continue
                
                frame = frame_queue.get()
                current_time = time.time()
                
                # ä½¿ç”¨é”ä¿æŠ¤ç»“æœé˜Ÿåˆ—çš„è¯»å–æ“ä½œå’Œæ£€æµ‹å¯¹è±¡çš„æ›´æ–°
                new_objects = []
                with result_lock:
                    if not result_queue.empty():
                        # å–æœ€æ–°çš„ä¸€å¸§ç»“æœ
                        new_objects = result_queue.get()
                
                # ä½¿ç”¨é”ä¿æŠ¤æ£€æµ‹å¯¹è±¡çš„æ›´æ–°
                with objects_lock:
                    # å¦‚æœæœ‰æ–°æ£€æµ‹ç»“æœï¼Œæ›´æ–°ç¼“å­˜å’Œæ—¶é—´æˆ³
                    if new_objects:
                        print(f"æ£€æµ‹åˆ°ç›®æ ‡: {len(new_objects)}")
                        self.detected_objects = new_objects
                        self.last_detection_time = current_time
                    
                    # è¿‡æ»¤æ‰è¶…è¿‡æ˜¾ç¤ºæ—¶é—´çš„å¯¹è±¡
                    updated_objects = []
                    for box, label, conf, timestamp in self.detected_objects:
                        if current_time - timestamp <= BOX_DISPLAY_DURATION:
                            updated_objects.append((box, label, conf, timestamp))
                    
                    # æ›´æ–°è¿‡æ»¤åçš„å¯¹è±¡åˆ—è¡¨
                    self.detected_objects = updated_objects
                    
                    # å¤åˆ¶å½“å‰æ£€æµ‹å¯¹è±¡åˆ—è¡¨ä»¥é¿å…åœ¨ç»˜åˆ¶è¿‡ç¨‹ä¸­è¢«ä¿®æ”¹
                    current_detected_objects = self.detected_objects.copy()
                
                # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹åˆ°çš„å¯¹è±¡(æ–°æ£€æµ‹åˆ°çš„æˆ–ç¼“å­˜ä¸­çš„)
                for box, label, conf, timestamp in current_detected_objects:
                    # è®¡ç®—å‰©ä½™æ˜¾ç¤ºæ—¶é—´ï¼ˆç§’ï¼‰
                    remaining_time = BOX_DISPLAY_DURATION - (current_time - timestamp)
                    remaining_percent = remaining_time / BOX_DISPLAY_DURATION
                    
                    # æ ¹æ®å‰©ä½™æ—¶é—´ä¿®æ”¹é¢œè‰²é€æ˜åº¦
                    color_intensity = int(255 * min(1.0, remaining_percent * 2.0))  # ä½¿é¢œè‰²æ·¡å‡ºæ›´åŠ æ˜æ˜¾
                    box_color = (0, 0, color_intensity)  # çº¢è‰²ï¼Œå¼ºåº¦éšæ—¶é—´å‡å¼±
                    
                    # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ ‡ç­¾
                    x1, y1, x2, y2 = box
                    cv2.rectangle(frame, (x1, y1), (x2, y2), box_color, 2)
                    frame = trigger_alert(frame, f"{label} {conf:.2f}", box, color=(0, 0, color_intensity))
                
                # å‡†å¤‡æ€§èƒ½ç»Ÿè®¡
                with stats_lock:
                    stats_text = (
                        f"æ£€æµ‹é—´éš”: {DETECT_INTERVAL}å¸§\n"
                        f"YOLO FPS: {perf_stats['yolo_fps']}\n"
                        f"CLIP FPS: {perf_stats['clip_fps']}\n"
                        f"æ€»ä½“ FPS: {perf_stats['total_fps']}\n"
                        f"å¤„ç†å¯¹è±¡æ•°: {perf_stats['objects_processed']}\n"
                        f"è·³è¿‡çš„å¸§æ•°: {perf_stats['skipped_frames']}\n"
                        f"å½“å‰æ˜¾ç¤ºå¯¹è±¡: {len(current_detected_objects)}\n"
                        f"æ¡†æ˜¾ç¤ºæ—¶é—´: {BOX_DISPLAY_DURATION}ç§’\n"
                        f"æ¯å¸§å¤„ç†æ—¶é—´: {perf_stats['frame_process_time']}ç§’ ({perf_stats['last_frame_objects']}ä¸ªç›®æ ‡)\n"
                        f"æœ€åæ£€æµ‹æ—¶é—´: {time.strftime('%H:%M:%S', time.localtime(self.last_detection_time))}"
                    )
                
                # å°†ç»“æœæ”¾å…¥æ˜¾ç¤ºé˜Ÿåˆ—
                if not display_queue.full():
                    # é¢„å…ˆè½¬æ¢ä¸ºRGBï¼Œé¿å…åœ¨UIçº¿ç¨‹ä¸­è½¬æ¢
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    display_queue.put((rgb_frame, stats_text))
                
            except Exception as e:
                print(f"æ¸²æŸ“é”™è¯¯: {e}")
                time.sleep(0.1)

    def update_video(self):
        """æ›´æ–°è§†é¢‘æ˜¾ç¤º"""
        try:
            if not display_queue.empty():
                frame, stats = display_queue.get()
                
                # ç›´æ¥ä½¿ç”¨å·²è½¬æ¢ä¸ºRGBçš„å¸§åˆ›å»ºå›¾åƒ
                img = ImageTk.PhotoImage(Image.fromarray(frame))
                
                self.video_frame.configure(image=img)
                self.video_frame.image = img  # ä¿æŒå¼•ç”¨
                self.stats_var.set(stats)
        except Exception as e:
            print(f"è§†é¢‘æ›´æ–°é”™è¯¯: {e}")
            
        if not exit_event.is_set():
            self.root.after(40, self.update_video)  # 25fpsæ›´æ–°
    
    def on_close(self):
        exit_event.set()
        self.root.destroy()

def trigger_alert(frame, text, box, color=(0, 0, 255), alpha=1):
    x1, y1, x2, y2 = box
    return cv2_puttext_chinese(
        img=frame,
        text=text,
        position=(x1, y1 - 35),
        font_size=30,
        color=color,
        color_alpha=alpha,
    )

if __name__ == "__main__":
    app = MainApplication()
    app.root.mainloop()